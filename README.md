### Aim of the project

Our project proposal is to generate adversarial attack on neural network (i.e) Deep Learning
(DL) system using mutation based fuzzer. DL systems are of great importance in the current
scenario as there is an emerging usage of autonomous systems in many safety critical environment that does image recognition, pattern recognition and predictions. However the DL systems
are vulnerable to well-formed inputs which it cannot perceive as a potential attack. Testing the
DL systems is mandatory to secure and preserve the robustness of the same. The ultimate goal
of a mutation fuzzer causing the neural network to misclassify the target data is to achieve
maximal neural coverage with minor perturbations to the input data. The mutated input image(Adversarial examples) are specialised inputs that is generated with respect to the neural
network under test with the purpose of confusing the model .As a part of the project we are
generating different adversarial attacks on different state of art neural networks and analyzing
the metrics of epsilon(minimal noise threshold),accuracy and loss of the model along with F1-
Score,precision and recall.Our contribution and approach to this problem is to develop a neural
network that is by default robust to the adversarial attacks generated even without the general
approach of training the model on adversarial inputs.
